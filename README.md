# llms-and-puzzles

Large Language Models (LLMs) became the craze in recent years. They are incredibly fluent and are able to produce high quality texts in every domain they are applied to. In addition to that, Emergent Abilities of Large Language Models demonstrate that with the growth in size, new abilities like modular arithmetic show up in LLMs. Those abilities cannot be simply extrapolated from the performance of smaller models, so that raises the question of exactly how intelligent LLMs are. Which tasks are they capable of, and which tasks are too hard for them? If there is a limit to LLMs capabilities - can growing the number of parameters overcome it? If the latter exist, is growing the size of LLMs even more - the solution?

Through my masters' thesis, I aim to push the boundaries of LLMs capabilities with challenging tasks. Puzzles of different sorts lie in the intersection of reasoning and linguistic (or meta-linguistic) abilities. In this research, I am exploring 3 major types of puzzles: Rosetta Stone puzzles from Linguistic Olympiads, logic puzzles and cryptic crosswords. The variety of puzzle types allows us to look at different aspects of the abilities in question.